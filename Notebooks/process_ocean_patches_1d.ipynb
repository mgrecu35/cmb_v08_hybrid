{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import torch\n",
    "fp=glob.glob('output_ocean_patches/patches_*.nc')\n",
    "fp=sorted(fp)\n",
    "land_fractL=[]\n",
    "tc_L=[]\n",
    "sfc_type_L=[]\n",
    "sk_temp_L=[]\n",
    "sk_temp1_L=[]\n",
    "xenc_L=[]\n",
    "xenc_prec_L=[]\n",
    "oe_wvp_L=[]\n",
    "near_sfc_precip_L=[]\n",
    "sfc_emiss_L=[]\n",
    "cldw_oe_L=[]\n",
    "sfc_bin_L=[]\n",
    "qv_L=[]\n",
    "import pickle\n",
    "\n",
    "#dense_autoencoder=torch.jit.load('dense_autoencoder.pt')\n",
    "#with open('scaler_qv.pkl','wb') as f:\n",
    "#    pickle.dump(scaler_qv,f)\n",
    "#scripted_model = torch.jit.script(model)\n",
    "#scripted_model.save('dense_autoencoder.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129/129 [00:31<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700650, 13)\n"
     ]
    }
   ],
   "source": [
    "tc_L=[]\n",
    "sfc_type_L=[]\n",
    "sk_temp_L=[]\n",
    "sk_temp1_L=[]\n",
    "xenc_L=[]\n",
    "x_env_enc_L=[]\n",
    "xenc_prec_L=[]\n",
    "oe_wvp_L=[]\n",
    "near_sfc_precip_L=[]\n",
    "sfc_emiss_L=[]\n",
    "cldw_oe_L=[]\n",
    "sfc_bin_L=[]\n",
    "qv_L=[]\n",
    "from numba import jit\n",
    "@jit(nopython=True)\n",
    "def get_qv_sfc(qv,sfc_bin,qv_sfc):\n",
    "    n_patches,nscans,nrays,nlevs=qv.shape\n",
    "    for i in range(n_patches):\n",
    "        for j in range(nscans):\n",
    "            for k in range(nrays):\n",
    "                qv_sfc[i,j,k]=qv[i,j,k,sfc_bin[i,j,k]]\n",
    "    return qv_sfc\n",
    "import tqdm\n",
    "for f in tqdm.tqdm(fp[::]):\n",
    "    with nc.Dataset(f) as df:\n",
    "        tc_s1=df['tc_s1'][:]\n",
    "        tc_s2=df['tc_s2'][:]\n",
    "        surface_type=df['surface_type'][:]\n",
    "        sk_temp=df['sk_temp'][:]\n",
    "        qv=df['qv'][:]\n",
    "        qv_oe=df['qv_oe'][:]\n",
    "        cldw_oe=df['cldw_oe'][:]\n",
    "        oe_wvp=df['oe_wvp'][:]\n",
    "        near_sfc_precip=df['near_sfc_precip'][:]\n",
    "        sfc_emiss=df['sfc_emiss'][:]\n",
    "        sfc_bin=df['sfc_bin'][:]\n",
    "        x_enc_prec=df['x_enc'][:]\n",
    "        #print(tc_s1.shape)\n",
    "        n_patches=tc_s1.shape[0]\n",
    "        qv_sfc=get_qv_sfc(qv.data,sfc_bin.data,np.zeros((n_patches,tc_s1.shape[1],tc_s1.shape[2]),dtype=np.float32))\n",
    "        #print(qv_sfc.shape)\n",
    "        #print(qv_sfc.shape)\n",
    "        for i in range(n_patches):\n",
    "            a_nan=np.isnan(near_sfc_precip[i,:,24].data)\n",
    "            if sum(a_nan) > 0:\n",
    "                continue\n",
    "            if near_sfc_precip[i,:,24].data.min() <0:\n",
    "                continue\n",
    "            if oe_wvp[i,24].min() <0:\n",
    "                continue\n",
    "            if tc_s1[i,:,24,:].min() <0:\n",
    "                continue\n",
    "            if tc_s2[i,:,24,:].min() <0:\n",
    "                continue\n",
    "            tc_L.append(np.concatenate((tc_s1[i,:,24,:],tc_s2[i,:,24,:]),axis=-1))\n",
    "            sfc_type_L.append(surface_type[i,:,24])\n",
    "            sk_temp_L.append(sk_temp[i,:,24])\n",
    "            qv_L.append(qv_sfc[i,:,24])\n",
    "        #xenc_L.append(x_enc[i,:,:,:])\n",
    "        #x_env_enc_L.append(x_env_enc[i,:,:,:])\n",
    "            xenc_prec_L.append(x_enc_prec[i,:,24,:])\n",
    "            oe_wvp_L.append(oe_wvp[i,:,24])\n",
    "            sfc_emiss_L.append(sfc_emiss[i,:,24,:])\n",
    "            log_precip=np.log10(1+near_sfc_precip[i,:,24].data/0.1)\n",
    "            a_nan=np.nonzero(log_precip!=log_precip)\n",
    "            if len(a_nan[0]) > 0:\n",
    "                stop\n",
    "            near_sfc_precip_L.append(np.log10(1+near_sfc_precip[i,:,24]/0.1))\n",
    "        #break\n",
    "    #break\n",
    "tc_L=np.concatenate(tc_L,axis=0)\n",
    "sfc_type_L=np.concatenate(sfc_type_L,axis=0)\n",
    "sk_temp_L=np.concatenate(sk_temp_L,axis=0)\n",
    "xenc_prec_L=np.concatenate(xenc_prec_L,axis=0)\n",
    "oe_wvp_L=np.concatenate(oe_wvp_L,axis=0)\n",
    "sfc_emiss_L=np.concatenate(sfc_emiss_L,axis=0)\n",
    "near_sfc_precip_L=np.concatenate(near_sfc_precip_L,axis=0)\n",
    "qv_L=np.concatenate(qv_L,axis=0)\n",
    "print(tc_L.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.30909017\n",
      "(700650,) (700650,)\n",
      "[[1.         0.93872849]\n",
      " [0.93872849 1.        ]]\n",
      "0 324\n",
      "(677160,)\n",
      "(23490,)\n"
     ]
    }
   ],
   "source": [
    "print(qv_L.min(),oe_wvp_L.min())\n",
    "print(qv_L.shape,oe_wvp_L.shape)\n",
    "\n",
    "print(np.corrcoef(qv_L,oe_wvp_L))\n",
    "print(sfc_type_L.min(),sfc_type_L.max())\n",
    "a=np.nonzero(sfc_type_L==0)\n",
    "print(a[0].shape)\n",
    "b=np.nonzero(sfc_type_L!=0)\n",
    "print(b[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3.1356606 0.047525432\n",
      "(886392,)\n"
     ]
    }
   ],
   "source": [
    "print(near_sfc_precip_L.min(),near_sfc_precip_L.max(),near_sfc_precip_L.mean())\n",
    "print(near_sfc_precip_L.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "tcX=xr.DataArray(tc_L,dims=['n_profiles','n_chans'],coords={'n_profiles':np.arange(tc_L.shape[0]),'n_chans':np.arange(tc_L.shape[1])})\n",
    "sfc_typeX=xr.DataArray(sfc_type_L,dims=['n_profiles'],coords={'n_profiles':np.arange(sfc_type_L.shape[0])})\n",
    "sk_tempX=xr.DataArray(sk_temp_L,dims=['n_profiles'],coords={'n_profiles':np.arange(sk_temp_L.shape[0])})\n",
    "qvX=xr.DataArray(qv_L,dims=['n_profiles'],coords={'n_profiles':np.arange(qv_L.shape[0])})\n",
    "oe_wvpX=xr.DataArray(oe_wvp_L,dims=['n_profiles'],coords={'n_profiles':np.arange(oe_wvp_L.shape[0])})\n",
    "near_sfc_precipX=xr.DataArray(near_sfc_precip_L,dims=['n_profiles'],coords={'n_profiles':np.arange(near_sfc_precip_L.shape[0])})\n",
    "x_enc_precX=xr.DataArray(xenc_prec_L,dims=['n_profiles','n_comp'],coords={'n_profiles':np.arange(xenc_prec_L.shape[0]),'n_comp':np.arange(xenc_prec_L.shape[1])})\n",
    "ds=xr.Dataset({'tc':tcX,'sfc_type':sfc_typeX,'sk_temp':sk_tempX,'qv':qvX,'oe_wvp':oe_wvpX,'near_sfc_precip':near_sfc_precipX,'x_enc_prec':x_enc_precX})\n",
    "comp = dict(zlib=True, complevel=5)\n",
    "encoding = {var: comp for var in ds.data_vars}\n",
    "ds.to_netcdf('training_1d_data_ocean.nc',encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tc': [array([172.58295, 101.09319, 194.71022, 132.48866, 219.26488, 218.11345,\n",
      "       163.62964, 257.93765, 229.89326, 271.30518, 266.49127, 258.22556,\n",
      "       266.8596 ], dtype=float32), array([20.9655  , 32.31992 , 19.623337, 31.60958 , 22.2808  , 14.723263,\n",
      "       27.47244 , 15.023948, 27.124723, 13.967746, 18.726631,  8.970008,\n",
      "       10.455282], dtype=float32)], 'sfc_type': [4.4636551773353315, 25.114096223611075], 'sk_temp': [288.79288, 11.261115], 'qv': [7.849633, 4.943822], 'oe_wvp': [24.916843, 17.580933], 'near_sfc_precip': [0.07530318, 0.28768516], 'x_enc_prec': [array([-0.02755156, -0.01208592,  0.02310711, -0.05463355,  0.02173839,\n",
      "        0.03918647]), array([0.29761569, 0.24226758, 0.2344908 , 0.38282115, 0.29231676,\n",
      "       0.28413181])]}\n"
     ]
    }
   ],
   "source": [
    "d_scaler={}\n",
    "for vars in ['tc','sfc_type','sk_temp','qv','oe_wvp','near_sfc_precip','x_enc_prec']:\n",
    "    #print(vars,ds[vars][:].mean(axis=0),ds[vars][:].std(axis=0))\n",
    "    d_scaler[vars]=[ds[vars][:].data.mean(axis=0),ds[vars][:].data.std(axis=0)]\n",
    "with open('scaler_1d_ocean.pkl','wb') as f:\n",
    "    pickle.dump(d_scaler,f)\n",
    "\n",
    "# write the d_scaler to a text file easy to read in Fortran\n",
    "with open('scaler_1d_ocean.txt', 'w') as f:\n",
    "    for key, value in d_scaler.items():\n",
    "        f.write(\"%s\\n\"%key)\n",
    "        mean_text=''\n",
    "        try:\n",
    "            for v in value[0]:\n",
    "                mean_text+=str(v)+' '\n",
    "        except:\n",
    "            mean_text+=str(value[0])+' '\n",
    "        f.write(\"%s\\n\"%mean_text)\n",
    "        std_text=''\n",
    "        try:\n",
    "            for v in value[1]:\n",
    "                std_text+=str(v)+' '\n",
    "        except:\n",
    "            std_text+=str(value[1])+' '\n",
    "        f.write(\"%s\\n\"%std_text)\n",
    "f.close()\n",
    "print(d_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111.37076710981147, 44.74047868686988]\n",
      "sfc_type\n"
     ]
    }
   ],
   "source": [
    "print(value)\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700650, 6)\n",
      "(700650,)\n",
      "(700650,)\n"
     ]
    }
   ],
   "source": [
    "tc_scaled=(tcX.data-d_scaler['tc'][0])/d_scaler['tc'][1]\n",
    "sfc_type_scaled=(sfc_typeX.data-d_scaler['sfc_type'][0])/d_scaler['sfc_type'][1]\n",
    "sk_temp_scaled=(sk_tempX.data-d_scaler['sk_temp'][0])/d_scaler['sk_temp'][1]\n",
    "qv_scaled=(qvX.data-d_scaler['qv'][0])/d_scaler['qv'][1]\n",
    "oe_wvp_scaled=(oe_wvpX.data-d_scaler['oe_wvp'][0])/d_scaler['oe_wvp'][1]\n",
    "near_sfc_precip_scaled=(near_sfc_precipX.data-d_scaler['near_sfc_precip'][0])/d_scaler['near_sfc_precip'][1]\n",
    "x_enc_prec_scaled=(x_enc_precX.data-d_scaler['x_enc_prec'][0])/d_scaler['x_enc_prec'][1]\n",
    "print(x_enc_prec_scaled.shape)\n",
    "print(near_sfc_precip_scaled.shape)\n",
    "print(near_sfc_precip_L.shape)\n",
    "\n",
    "X_input=torch.tensor(np.concatenate((tc_scaled,sfc_type_scaled[:,np.newaxis],sk_temp_scaled[:,np.newaxis],qv_scaled[:,np.newaxis]),axis=-1),dtype=torch.float32)\n",
    "y_output=torch.tensor(np.concatenate((oe_wvp_scaled[:,np.newaxis],near_sfc_precip_scaled[:,np.newaxis],x_enc_prec_scaled),axis=-1),dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntot=X_input.shape[0]\n",
    "n_train=int(0.8*ntot)\n",
    "import torch\n",
    "X_train=torch.utils.data.TensorDataset(X_input[:n_train],y_output[:n_train])\n",
    "X_test=torch.utils.data.TensorDataset(X_input[n_train:],y_output[n_train:])\n",
    "X_train_loader = torch.utils.data.DataLoader(X_train, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DenseModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 128)\n",
    "        self.dropout1 = torch.nn.Dropout(p=0.05)\n",
    "        self.fc2 = torch.nn.Linear(128, 64)\n",
    "        self.dropout2 = torch.nn.Dropout(p=0.05)\n",
    "        self.fc3 = torch.nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        #x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        #x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "input_size = X_input.shape[1]\n",
    "output_size = y_output.shape[1]\n",
    "model = DenseModel(input_size, output_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "num_epochs = 50\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.4442\n",
      "Epoch [2/20], Loss: 0.4111\n",
      "Epoch [3/20], Loss: 0.4028\n",
      "Epoch [4/20], Loss: 0.3987\n",
      "Epoch [5/20], Loss: 0.3915\n",
      "Epoch [6/20], Loss: 0.3895\n",
      "Epoch [7/20], Loss: 0.3877\n",
      "Epoch [8/20], Loss: 0.3842\n",
      "Epoch [9/20], Loss: 0.3809\n",
      "Epoch [10/20], Loss: 0.3822\n",
      "Epoch [11/20], Loss: 0.3791\n",
      "Epoch [12/20], Loss: 0.3778\n",
      "Epoch [13/20], Loss: 0.3772\n",
      "Epoch [14/20], Loss: 0.3778\n",
      "Epoch [15/20], Loss: 0.3743\n",
      "Epoch [16/20], Loss: 0.3732\n",
      "Epoch [17/20], Loss: 0.3729\n",
      "Epoch [18/20], Loss: 0.3718\n",
      "Epoch [19/20], Loss: 0.3700\n",
      "Epoch [20/20], Loss: 0.3703\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = 0\n",
    "    icount=0\n",
    "    for i, (inputs, targets) in enumerate(X_train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss+=loss.item()\n",
    "        icount+=1\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss/icount:.4f}')\n",
    "torch.save(model.state_dict(), 'dense_model_land.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input : Float(1, 16, strides=[16, 1], requires_grad=0, device=cpu),\n",
      "      %fc1.weight : Float(128, 16, strides=[16, 1], requires_grad=1, device=cpu),\n",
      "      %fc1.bias : Float(128, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc2.weight : Float(64, 128, strides=[128, 1], requires_grad=1, device=cpu),\n",
      "      %fc2.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc3.weight : Float(8, 64, strides=[64, 1], requires_grad=1, device=cpu),\n",
      "      %fc3.bias : Float(8, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %/fc1/Gemm_output_0 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc1/Gemm\"](%input, %fc1.weight, %fc1.bias), scope: __main__.DenseModel::/torch.nn.modules.linear.Linear::fc1 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/Relu_output_0 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/Relu\"](%/fc1/Gemm_output_0), scope: __main__.DenseModel:: # /var/folders/x_/d2_jzyq50052xh1_tk02bnmc0000gq/T/ipykernel_79126/1304595632.py:11:0\n",
      "  %/fc2/Gemm_output_0 : Float(1, 64, strides=[64, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc2/Gemm\"](%/Relu_output_0, %fc2.weight, %fc2.bias), scope: __main__.DenseModel::/torch.nn.modules.linear.Linear::fc2 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/Relu_1_output_0 : Float(1, 64, strides=[64, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/Relu_1\"](%/fc2/Gemm_output_0), scope: __main__.DenseModel:: # /var/folders/x_/d2_jzyq50052xh1_tk02bnmc0000gq/T/ipykernel_79126/1304595632.py:13:0\n",
      "  %output : Float(1, 8, strides=[8, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc3/Gemm\"](%/Relu_1_output_0, %fc3.weight, %fc3.bias), scope: __main__.DenseModel::/torch.nn.modules.linear.Linear::fc3 # /Users/mgrecu/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "x_test=torch.randn(1, input_size)\n",
    "torch.onnx.export(model, x_test, 'GMI_ONNX_Models/xdense_model_ocean.onnx', verbose=True,  input_names=['input'], output_names=['output'])\n",
    "#torch.onnx.export(dense_autoencoder.encoder, test_tensor, \"GMI_ONNX_Models/dense_encoder_land.onnx\", verbose=True, input_names = ['input'], output_names = ['enc_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.91375506]\n",
      " [0.91375506 1.        ]]\n",
      "[[1.         0.77477835]\n",
      " [0.77477835 1.        ]]\n",
      "[[1.         0.78943773]\n",
      " [0.78943773 1.        ]]\n",
      "[[1.         0.70583074]\n",
      " [0.70583074 1.        ]]\n",
      "[[1.        0.6388462]\n",
      " [0.6388462 1.       ]]\n",
      "[[1.         0.80221772]\n",
      " [0.80221772 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "y_pred=model(X_input[n_train:]).detach().numpy()\n",
    "y_test=y_output[n_train:].numpy()\n",
    "for i in range(6):\n",
    "    print(np.corrcoef(y_pred[:,i],y_test[:,i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
