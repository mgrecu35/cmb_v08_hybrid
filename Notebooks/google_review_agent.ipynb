{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could you review this text? Do not provide explantions or suggestions, just an edited version of the text.\n",
      "\n",
      "As apparent in Fig. 1, the TMRW-R1 reflectivity observations appear realistic and qualitatively consistent with the GPM Ka-band radar observations.  However, only a rigorous and systematic evaluation can rule out the potential existence of artifacts and biases in the TMRW-R1 products.\n",
      "To address this need, we propose the development of a machine learning (ML) methodology that leverages the GPM data to learn the multivariate distributions the Ka-band radar observations as well as the relationships between observed reflectivity profiles and associated precipitation and uncertainties. This methodology will be used to evaluate the reliability (or “health”) of the observed TMRW-R1 Ka-band radar observations and the uncertainties of the precipitation estimates derived from them.  The methodology will be also used to identify the conditions under which the Ka-band radar observations are most useful. This will be achieved via the uncertainty quantification component using explainable AI techniques that will enable the attribution of uncertainties to incomplete (or missing) information in the observations. \n",
      "Specifically, we will:\n",
      "*\tDevelop an autoencoder model that will learn to represent Ka-band space-borne radar observations into a compressed space and apply it to TMRW-R1 observations to investigate the potential existence of anomalous observations. \n",
      "*\tDeveloped an ML precipitation algorithm using GPM data to learn the relationships between the Ka-band radar observations and precipitation rather from the GPM Combined Radar Radiometer Algorithm (CORRA) and quantify their uncertainties.\n",
      "*\tApply the ML algorithm to Tomorrow.io Ka-band radar observations and evaluate the consistency of the TMRW-R1 precipitation product with the ML precipitation estimates and associated uncertainties.\n",
      "*\tAnalyze the results using explainable AI techniques to understand the relationship between the Ka-band radar observations (from both Tomorrow.io and the GPM radar) and the associated precipitation.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "prompt=\"\"\"\n",
    "Could you review this text? Do not provide explantions or suggestions, just an edited version of the text.\n",
    "\n",
    "As apparent in Fig. 1, the TMRW-R1 reflectivity observations appear realistic and qualitatively consistent with the GPM Ka-band radar observations.  However, only a rigorous and systematic evaluation can rule out the potential existence of artifacts and biases in the TMRW-R1 products.\n",
    "To address this need, we propose the development of a machine learning (ML) methodology that leverages the GPM data to learn the multivariate distributions the Ka-band radar observations as well as the relationships between observed reflectivity profiles and associated precipitation and uncertainties. This methodology will be used to evaluate the reliability (or “health”) of the observed TMRW-R1 Ka-band radar observations and the uncertainties of the precipitation estimates derived from them.  The methodology will be also used to identify the conditions under which the Ka-band radar observations are most useful. This will be achieved via the uncertainty quantification component using explainable AI techniques that will enable the attribution of uncertainties to incomplete (or missing) information in the observations. \n",
    "Specifically, we will:\n",
    "*\tDevelop an autoencoder model that will learn to represent Ka-band space-borne radar observations into a compressed space and apply it to TMRW-R1 observations to investigate the potential existence of anomalous observations. \n",
    "*\tDeveloped an ML precipitation algorithm using GPM data to learn the relationships between the Ka-band radar observations and precipitation rather from the GPM Combined Radar Radiometer Algorithm (CORRA) and quantify their uncertainties.\n",
    "*\tApply the ML algorithm to Tomorrow.io Ka-band radar observations and evaluate the consistency of the TMRW-R1 precipitation product with the ML precipitation estimates and associated uncertainties.\n",
    "*\tAnalyze the results using explainable AI techniques to understand the relationship between the Ka-band radar observations (from both Tomorrow.io and the GPM radar) and the associated precipitation.\n",
    "\n",
    "\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=\"AIzaSyBypAsQjSjEqQLlojrYFSvt-tCVgReDRgo\")\n",
    "def ask_question(prompt):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\", contents=prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import ollama\n",
    "model_name = \"mistral\"\n",
    "response = ollama.generate(\n",
    "    model=model_name,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='mistral' created_at='2025-04-01T13:58:29.239653Z' done=True done_reason='stop' total_duration=13082623959 load_duration=22153459 prompt_eval_count=470 prompt_eval_duration=161000000 eval_count=415 eval_duration=12898000000 response=' As shown in Fig. 1, the TMRW-R1 reflectivity observations appear realistic and qualitatively consistent with the GPM Ka-band radar observations. However, a thorough and systematic evaluation is necessary to eliminate potential artifacts and biases in the TMRW-R1 products.\\n\\nTo meet this requirement, we suggest the creation of a machine learning (ML) methodology that utilizes GPM data to learn the multivariate distributions of the Ka-band radar observations as well as the relationships between observed reflectivity profiles and associated precipitation and uncertainties. This methodology will be employed to assess the reliability (or \"health\") of the TMRW-R1 Ka-band radar observations and the uncertainties in the derived precipitation estimates. Additionally, it will identify the conditions under which the Ka-band radar observations are most beneficial. This will be accomplished through the uncertainty quantification component using explainable AI techniques that will attribute uncertainties to missing or incomplete information in the observations.\\n\\nSpecifically, we will:\\n\\n* Develop an autoencoder model that learns to represent Ka-band space-borne radar observations in a reduced dimension and apply it to TMRW-R1 observations to explore the presence of anomalous observations.\\n* Create an ML precipitation algorithm using GPM data to learn the relationships between the Ka-band radar observations and precipitation, rather than from the GPM Combined Radar Radiometer Algorithm (CORRA), and quantify their uncertainties.\\n* Utilize the ML algorithm on Tomorrow.io Ka-band radar observations and compare the consistency of the TMRW-R1 precipitation product with the ML precipitation estimates and associated uncertainties.\\n* Analyze the results using explainable AI techniques to comprehend the relationship between the Ka-band radar observations (from both Tomorrow.io and the GPM radar) and the related precipitation.' context=[3, 1027, 781, 17792, 1136, 4826, 1224, 3013, 29572, 3146, 1227, 3852, 2717, 1208, 1362, 1210, 18046, 29493, 1544, 1164, 20295, 3519, 1070, 1040, 3013, 29491, 781, 781, 2966, 8334, 1065, 5594, 29491, 29473, 29508, 29493, 1040, 1088, 16994, 29548, 29501, 29522, 29508, 8735, 3342, 14643, 5073, 20363, 1072, 4877, 1047, 7076, 11420, 1163, 1040, 1188, 9365, 16889, 29501, 4553, 25719, 14643, 29491, 29473, 3761, 29493, 1633, 1032, 11304, 15906, 1072, 26992, 15965, 1309, 6686, 1343, 1040, 5396, 9793, 1070, 2292, 25407, 1072, 5008, 2786, 1065, 1040, 1088, 16994, 29548, 29501, 29522, 29508, 4844, 29491, 781, 2319, 3730, 1224, 1695, 29493, 1246, 20101, 1040, 4867, 1070, 1032, 6367, 5936, 1093, 4595, 29499, 2806, 2929, 1137, 23367, 2059, 1040, 1188, 9365, 1946, 1066, 3590, 1040, 3299, 1217, 2668, 1148, 21547, 1040, 16889, 29501, 4553, 25719, 14643, 1158, 1930, 1158, 1040, 10159, 2212, 8743, 8735, 3342, 20495, 1072, 6131, 28408, 5942, 1072, 1289, 9190, 2458, 1265, 29491, 1619, 2806, 2929, 1390, 1115, 2075, 1066, 16395, 1040, 23657, 3205, 1093, 1039, 1749, 15281, 28289, 1070, 1040, 8743, 1088, 16994, 29548, 29501, 29522, 29508, 16889, 29501, 4553, 25719, 14643, 1072, 1040, 1289, 9190, 2458, 1265, 1070, 1040, 28408, 5942, 17206, 12004, 1245, 1474, 29491, 29473, 1183, 2806, 2929, 1390, 1115, 1603, 2075, 1066, 9819, 1040, 5099, 1684, 1458, 1040, 16889, 29501, 4553, 25719, 14643, 1228, 1848, 6625, 29491, 1619, 1390, 1115, 12779, 4981, 1040, 18878, 5432, 3268, 6409, 2181, 8050, 1290, 16875, 10572, 1137, 1390, 9002, 1040, 19864, 1064, 1070, 1289, 9190, 2458, 1265, 1066, 1065, 11960, 1093, 1039, 7693, 29499, 2639, 1065, 1040, 14643, 29491, 29473, 781, 5715, 1689, 1346, 29493, 1246, 1390, 29515, 781, 29504, 780, 29525, 4211, 1178, 1164, 4795, 21555, 2997, 1137, 1390, 3590, 1066, 3672, 16889, 29501, 4553, 3532, 29501, 23363, 25719, 14643, 1546, 1032, 1391, 10793, 3532, 1072, 6348, 1146, 1066, 1088, 16994, 29548, 29501, 29522, 29508, 14643, 1066, 17982, 1040, 5396, 9793, 1070, 25329, 15758, 14643, 29491, 29473, 781, 29504, 780, 29525, 4211, 12184, 1164, 19771, 28408, 5942, 10232, 2181, 1188, 9365, 1946, 1066, 3590, 1040, 10159, 2212, 1040, 16889, 29501, 4553, 25719, 14643, 1072, 28408, 5942, 3978, 1245, 1040, 1188, 9365, 20190, 2079, 7212, 1051, 7212, 29478, 12366, 1744, 7201, 1093, 29511, 1785, 6012, 29499, 1072, 5432, 2343, 1420, 1289, 9190, 2458, 1265, 29491, 781, 29504, 780, 22365, 1040, 19771, 10232, 1066, 5428, 7068, 29491, 1459, 16889, 29501, 4553, 25719, 14643, 1072, 16395, 1040, 23842, 1070, 1040, 1088, 16994, 29548, 29501, 29522, 29508, 28408, 5942, 2861, 1163, 1040, 19771, 28408, 5942, 17206, 1072, 6131, 1289, 9190, 2458, 1265, 29491, 781, 29504, 780, 28322, 2142, 1040, 3671, 2181, 8050, 1290, 16875, 10572, 1066, 3148, 1040, 4526, 2212, 1040, 16889, 29501, 4553, 25719, 14643, 1093, 3979, 2328, 5428, 7068, 29491, 1459, 1072, 1040, 1188, 9365, 25719, 29499, 1072, 1040, 6131, 28408, 5942, 29491, 781, 781, 4, 1027, 1904, 5662, 1065, 5594, 29491, 29473, 29508, 29493, 1040, 1088, 16994, 29548, 29501, 29522, 29508, 8735, 3342, 14643, 5073, 20363, 1072, 4877, 1047, 7076, 11420, 1163, 1040, 1188, 9365, 16889, 29501, 4553, 25719, 14643, 29491, 3761, 29493, 1032, 13923, 1072, 26992, 15965, 1117, 5660, 1066, 21313, 5396, 2292, 25407, 1072, 5008, 2786, 1065, 1040, 1088, 16994, 29548, 29501, 29522, 29508, 4844, 29491, 781, 781, 2319, 3415, 1224, 16937, 29493, 1246, 4165, 1040, 10081, 1070, 1032, 6367, 5936, 1093, 4595, 29499, 2806, 2929, 1137, 5247, 5772, 1188, 9365, 1946, 1066, 3590, 1040, 3299, 1217, 2668, 1148, 21547, 1070, 1040, 16889, 29501, 4553, 25719, 14643, 1158, 1930, 1158, 1040, 10159, 2212, 8743, 8735, 3342, 20495, 1072, 6131, 28408, 5942, 1072, 1289, 9190, 2458, 1265, 29491, 1619, 2806, 2929, 1390, 1115, 15443, 1066, 8852, 1040, 23657, 3205, 1093, 1039, 1113, 15281, 2011, 1070, 1040, 1088, 16994, 29548, 29501, 29522, 29508, 16889, 29501, 4553, 25719, 14643, 1072, 1040, 1289, 9190, 2458, 1265, 1065, 1040, 12004, 28408, 5942, 17206, 29491, 17337, 29493, 1146, 1390, 9819, 1040, 5099, 1684, 1458, 1040, 16889, 29501, 4553, 25719, 14643, 1228, 1848, 21926, 29491, 1619, 1390, 1115, 20909, 1827, 1040, 18878, 5432, 3268, 6409, 2181, 8050, 1290, 16875, 10572, 1137, 1390, 7604, 1289, 9190, 2458, 1265, 1066, 7693, 1210, 1065, 11960, 2639, 1065, 1040, 14643, 29491, 781, 781, 5715, 1689, 1346, 29493, 1246, 1390, 29515, 781, 781, 29504, 9355, 1164, 4795, 21555, 2997, 1137, 3590, 29481, 1066, 3672, 16889, 29501, 4553, 3532, 29501, 23363, 25719, 14643, 1065, 1032, 10165, 10479, 1072, 6348, 1146, 1066, 1088, 16994, 29548, 29501, 29522, 29508, 14643, 1066, 12186, 1040, 7471, 1070, 25329, 15758, 14643, 29491, 781, 29504, 6438, 1164, 19771, 28408, 5942, 10232, 2181, 1188, 9365, 1946, 1066, 3590, 1040, 10159, 2212, 1040, 16889, 29501, 4553, 25719, 14643, 1072, 28408, 5942, 29493, 3978, 1589, 1245, 1040, 1188, 9365, 20190, 2079, 7212, 1051, 7212, 29478, 12366, 1744, 7201, 1093, 29511, 1785, 6012, 1325, 1072, 5432, 2343, 1420, 1289, 9190, 2458, 1265, 29491, 781, 29504, 16925, 1421, 1040, 19771, 10232, 1124, 5428, 7068, 29491, 1459, 16889, 29501, 4553, 25719, 14643, 1072, 10352, 1040, 23842, 1070, 1040, 1088, 16994, 29548, 29501, 29522, 29508, 28408, 5942, 2861, 1163, 1040, 19771, 28408, 5942, 17206, 1072, 6131, 1289, 9190, 2458, 1265, 29491, 781, 29504, 1862, 9678, 2142, 1040, 3671, 2181, 8050, 1290, 16875, 10572, 1066, 11483, 13437, 1040, 4526, 2212, 1040, 16889, 29501, 4553, 25719, 14643, 1093, 3979, 2328, 5428, 7068, 29491, 1459, 1072, 1040, 1188, 9365, 25719, 29499, 1072, 1040, 5970, 28408, 5942, 29491]\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Are there any unpaired data approaches that enables the comparison of two different datasets?  Meaning that I want to determine whether the two datasets are from the same multivariate distribution or not.\n",
    "\"\"\"\n",
    "response=ask_question(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, there are several unpaired data approaches that can be used to compare two different datasets and determine whether they are likely drawn from the same multivariate distribution. Here's a breakdown of common techniques, along with their strengths and weaknesses:\n",
      "\n",
      "**1. Statistical Tests (Classical Approach)**\n",
      "\n",
      "*   **Kolmogorov-Smirnov Test (K-S Test):**  The classic K-S test is for comparing *one-dimensional* distributions.  However, it can be applied *separately* to each dimension of your multivariate data.  A significant difference on even one dimension suggests the overall distributions are different.\n",
      "\n",
      "    *   **Strengths:** Simple to implement and interpret. Non-parametric (doesn't assume a specific distribution).\n",
      "    *   **Weaknesses:** Only suitable for univariate data.  Applying it to each dimension independently ignores potential dependencies between variables.  Adjusting p-values for multiple comparisons becomes crucial.  Can be less sensitive than other methods, especially for high-dimensional data.\n",
      "\n",
      "*   **Energy Distance Tests:** Energy distance tests compare the equality of distributions by computing the distance between the characteristic functions of the distributions.\n",
      "\n",
      "    *   **Strengths:** Can handle multivariate data and is non-parametric.  Consistent and powerful.\n",
      "    *   **Weaknesses:**  Can be computationally intensive for very large datasets.\n",
      "\n",
      "*   **Cramér-von Mises Test:**  Another statistical test for comparing the empirical cumulative distribution functions of two samples. Similar to K-S, it can be extended to multivariate data in some forms, though the extension is not as straightforward as the K-S test.\n",
      "    *   **Strengths:** More powerful than K-S in some situations.\n",
      "    *   **Weaknesses:** Still primarily designed for univariate data and has similar limitations to K-S when applied dimension-wise.\n",
      "\n",
      "**2. Distance-Based Methods (Non-Parametric)**\n",
      "\n",
      "*   **Maximum Mean Discrepancy (MMD):** MMD is a powerful kernel-based method that measures the distance between the means of the datasets in a reproducing kernel Hilbert space (RKHS). A small MMD suggests the distributions are similar.\n",
      "\n",
      "    *   **Strengths:**  Effective for comparing complex, high-dimensional distributions. Non-parametric. Can detect subtle differences in distributions.\n",
      "    *   **Weaknesses:**  Choice of kernel function is crucial and can affect performance. Computationally intensive for very large datasets.  Requires tuning a bandwidth parameter.  Hypothesis testing can be done via permutation tests.\n",
      "\n",
      "*   **Kernel Two-Sample Test:**  A hypothesis test based on kernel methods that uses a kernel function to measure the similarity between the two samples.  MMD is closely related to the kernel two-sample test.\n",
      "\n",
      "    *   **Strengths:**  Powerful for detecting differences between distributions. Non-parametric. Can handle high-dimensional data.\n",
      "    *   **Weaknesses:** Choice of kernel and parameters can affect performance. Computationally expensive for large datasets.\n",
      "\n",
      "*   **Sliced Wasserstein Distance (SWD):** Computes the Wasserstein distance between the distributions of projections of the data onto random directions. It approximates the Wasserstein distance, which is computationally expensive to compute directly in high dimensions.\n",
      "\n",
      "    *   **Strengths:** Can handle high-dimensional data relatively efficiently. More robust to outliers than other methods.\n",
      "    *   **Weaknesses:** Approximates the true Wasserstein distance, which could lead to some loss of accuracy. Performance depends on the number of random projections.\n",
      "\n",
      "**3. Machine Learning-Based Approaches**\n",
      "\n",
      "*   **Classifier-Based Two-Sample Tests:** Train a classifier (e.g., logistic regression, SVM, random forest) to distinguish between the two datasets.  If the classifier achieves high accuracy, it suggests the datasets are different.  The ROC AUC score can be a good indicator of separability.\n",
      "\n",
      "    *   **Strengths:** Flexible and can capture complex differences between distributions. Can handle high-dimensional data.\n",
      "    *   **Weaknesses:** Performance depends on the choice of classifier and its parameters. Susceptible to overfitting, especially with small datasets.\n",
      "\n",
      "*   **Adversarial Training (GAN-Based):** Use a Generative Adversarial Network (GAN). Train a generator on one dataset and a discriminator to distinguish between the generated data and the other dataset. If the discriminator can easily distinguish between the two, it suggests the distributions are different.\n",
      "\n",
      "    *   **Strengths:** Can capture very subtle differences. Can generate new samples from the learned distribution.\n",
      "    *   **Weaknesses:** Complex to train and requires significant computational resources.  Performance highly sensitive to GAN architecture and hyperparameters. Can be unstable and difficult to converge.\n",
      "\n",
      "*   **Density Ratio Estimation:** Estimate the ratio of the probability densities of the two datasets. If the ratio is close to 1, it suggests the distributions are similar. This can be done using techniques like KLIEP (Kullback-Leibler Importance Estimation Procedure).\n",
      "\n",
      "    *   **Strengths:** Provides a measure of similarity between distributions.  Can be used for other tasks like importance weighting.\n",
      "    *   **Weaknesses:**  Requires careful parameter tuning. Can be sensitive to outliers.\n",
      "\n",
      "**4. Visualization Techniques (Exploratory Analysis)**\n",
      "\n",
      "*   **Scatter Plots (pairwise):** For lower-dimensional data (2D or 3D), create scatter plots of each dataset and visually compare their shapes and densities.\n",
      "\n",
      "*   **Parallel Coordinate Plots:** Visualize high-dimensional data by drawing a line for each data point that connects its values across all dimensions. Compare the patterns of lines for the two datasets.\n",
      "\n",
      "*   **Principal Component Analysis (PCA):** Reduce the dimensionality of the data and visualize the data points in the PCA space. Compare the distributions of the two datasets in the reduced space.\n",
      "\n",
      "*   **t-distributed Stochastic Neighbor Embedding (t-SNE) / UMAP:**  Non-linear dimensionality reduction techniques that can reveal clusters and structures in high-dimensional data. Compare the resulting embeddings for the two datasets.\n",
      "\n",
      "**Key Considerations:**\n",
      "\n",
      "*   **Dimensionality:** The curse of dimensionality can make it challenging to compare distributions in high-dimensional spaces. Dimensionality reduction techniques can be helpful.\n",
      "*   **Sample Size:** Sufficient sample sizes are essential for most methods to provide reliable results.\n",
      "*   **Computational Cost:** Some methods, like MMD and GANs, can be computationally expensive, especially for large datasets.\n",
      "*   **Hypothesis Testing:** To formally test the hypothesis that the two datasets are drawn from the same distribution, you'll typically need to perform a hypothesis test based on the chosen method (e.g., using a permutation test with MMD).\n",
      "*   **Interpretability:** Some methods, like MMD, provide a score that indicates the dissimilarity between distributions. Others, like classifier-based tests, can be more difficult to interpret.  Visualization techniques can help in understanding the nature of any differences observed.\n",
      "*   **Data Preprocessing:** Ensure that both datasets are properly preprocessed, including scaling, normalization, and handling of missing values, as this can significantly affect the results.\n",
      "\n",
      "**How to Choose a Method:**\n",
      "\n",
      "1.  **Start with Visualization:** Explore the data using scatter plots, parallel coordinate plots, or dimensionality reduction techniques to get a sense of the potential differences.\n",
      "\n",
      "2.  **Consider Dimensionality:** For low-dimensional data, statistical tests or distance-based methods like MMD might be suitable. For high-dimensional data, consider techniques like kernel two-sample tests, sliced Wasserstein distance, or classifier-based tests.\n",
      "\n",
      "3.  **Evaluate Computational Cost:** Choose a method that is computationally feasible for your dataset size.\n",
      "\n",
      "4.  **Experiment and Compare:** Try multiple methods and compare their results to get a more robust assessment.\n",
      "\n",
      "5.  **Formal Hypothesis Testing:** If you need to formally test the hypothesis that the distributions are the same, perform a hypothesis test based on the chosen method.\n",
      "\n",
      "**Example using MMD (Python):**\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from sklearn.metrics.pairwise import rbf_kernel\n",
      "from scipy.stats import permutation_test\n",
      "\n",
      "def mmd(X, Y, kernel='rbf', gamma=1.0):\n",
      "    \"\"\"Calculates the Maximum Mean Discrepancy (MMD) between two datasets.\"\"\"\n",
      "    if kernel == 'rbf':\n",
      "        K_XX = rbf_kernel(X, X, gamma=gamma)\n",
      "        K_YY = rbf_kernel(Y, Y, gamma=gamma)\n",
      "        K_XY = rbf_kernel(X, Y, gamma=gamma)\n",
      "    else:\n",
      "        raise ValueError(\"Only RBF kernel is currently supported.\")\n",
      "\n",
      "    mmd_value = np.mean(K_XX) + np.mean(K_YY) - 2 * np.mean(K_XY)\n",
      "    return mmd_value\n",
      "\n",
      "def permutation_test_mmd(X, Y, n_permutations=1000, kernel='rbf', gamma=1.0, random_state=None):\n",
      "    \"\"\"Performs a permutation test to assess the significance of the MMD.\"\"\"\n",
      "\n",
      "    def statistic(X, Y):\n",
      "        return mmd(X, Y, kernel=kernel, gamma=gamma)\n",
      "\n",
      "    # Combine the data\n",
      "    data = np.vstack((X, Y))\n",
      "\n",
      "    # Define a function to permute the labels\n",
      "    def permuted_statistic(data, random_state):\n",
      "      idx = np.random.permutation(len(data))\n",
      "      X_permuted = data[idx[:len(X)]]\n",
      "      Y_permuted = data[idx[len(X):]]\n",
      "      return statistic(X_permuted, Y_permuted)\n",
      "\n",
      "    # Calculate the observed MMD\n",
      "    observed_mmd = statistic(X, Y)\n",
      "\n",
      "    # Perform the permutation test\n",
      "    rng = np.random.default_rng(random_state) # Use numpy's random number generator\n",
      "    permuted_mmds = [permuted_statistic(data, rng) for _ in range(n_permutations)]\n",
      "    p_value = np.mean(permuted_mmds >= observed_mmd)\n",
      "\n",
      "    return observed_mmd, p_value\n",
      "\n",
      "\n",
      "# Example usage\n",
      "np.random.seed(42)\n",
      "X = np.random.normal(loc=0, scale=1, size=(100, 2))  # Dataset 1\n",
      "Y = np.random.normal(loc=0.5, scale=1, size=(100, 2)) # Dataset 2 (slightly shifted)\n",
      "\n",
      "observed_mmd, p_value = permutation_test_mmd(X, Y, n_permutations=1000)\n",
      "\n",
      "print(f\"Observed MMD: {observed_mmd:.4f}\")\n",
      "print(f\"P-value: {p_value:.4f}\")\n",
      "\n",
      "if p_value < 0.05:\n",
      "    print(\"The distributions are significantly different (reject null hypothesis).\")\n",
      "else:\n",
      "    print(\"The distributions are not significantly different (fail to reject null hypothesis).\")\n",
      "```\n",
      "\n",
      "Remember to install necessary libraries: `pip install scikit-learn scipy`\n",
      "\n",
      "This is a starting point. The best method depends on the specifics of your data and the goals of your analysis.  Good luck!\n",
      "\n",
      "Yes, the Maximum Mean Discrepancy (MMD) method works for multivariate distributions. In fact, it's commonly used and often preferred for comparing multivariate distributions over methods that rely on comparing marginal distributions.\n",
      "\n",
      "Here's why and how:\n",
      "\n",
      "* **MMD is designed for general distributions:**  The fundamental idea behind MMD is to embed the distributions into a reproducing kernel Hilbert space (RKHS).  This embedding allows MMD to compare distributions without needing to assume any specific parametric form or independence structure.  The RKHS framework is general enough to handle distributions of arbitrary dimensionality, including multivariate distributions.\n",
      "\n",
      "* **Capturing dependencies:**  By considering the joint distribution through the kernel function, MMD implicitly captures dependencies between the variables.  Methods that compare marginal distributions separately (e.g., comparing histograms of each variable independently) fail to capture these dependencies, which can lead to inaccurate comparisons if the variables are correlated.  MMD, with a suitable kernel, considers these dependencies, providing a more comprehensive measure of similarity between the distributions.\n",
      "\n",
      "* **Kernels define the comparison:** The choice of kernel function is crucial. Common choices for multivariate data include:\n",
      "    * **Gaussian (RBF) kernel:** `k(x, y) = exp(-||x - y||^2 / (2 * sigma^2))`  where `x` and `y` are multivariate data points, `||.||` is the Euclidean norm, and `sigma` is the bandwidth.  This kernel is widely used due to its simplicity and ability to capture smooth relationships.\n",
      "    * **Linear kernel:** `k(x, y) = x^T y`  This is suitable if a linear relationship between the variables is expected.\n",
      "    * **Polynomial kernel:** `k(x, y) = (x^T y + c)^d`  Offers more flexibility than the linear kernel.\n",
      "    * **Laplacian kernel:** `k(x, y) = exp(-||x - y|| / sigma)`  Less sensitive to outliers compared to the Gaussian kernel.\n",
      "    * **More complex kernels:**  Domain-specific kernels can be designed to incorporate prior knowledge about the data.\n",
      "\n",
      "* **MMD as a distance between embeddings:** The MMD calculates the distance between the mean embeddings of the two distributions in the RKHS.  This distance reflects the overall difference between the distributions, considering all variables and their relationships.\n",
      "\n",
      "* **Advantages for multivariate data:**\n",
      "    * **Captures dependencies:** As mentioned earlier, this is a key advantage.\n",
      "    * **Non-parametric:**  Doesn't require assuming a specific distribution family.\n",
      "    * **General purpose:**  Applicable to a wide range of multivariate distributions.\n",
      "    * **Relatively easy to implement:**  Many libraries provide MMD implementations.\n",
      "\n",
      "* **Considerations:**\n",
      "    * **Kernel selection:** Choosing the right kernel is crucial for performance. Experimentation and domain knowledge are important.\n",
      "    * **Bandwidth selection:** For kernels like the Gaussian kernel, the bandwidth parameter `sigma` needs to be chosen carefully. Cross-validation or heuristic methods can be used.\n",
      "    * **Computational cost:**  The computational cost can be significant for large datasets, as it involves pairwise kernel calculations.\n",
      "\n",
      "**In summary, MMD is a well-established and effective method for comparing multivariate distributions. Its ability to capture dependencies between variables and its non-parametric nature make it a powerful tool for various applications, including hypothesis testing, generative model evaluation, and domain adaptation.**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)\n",
    "prompt=\"\"\"\n",
    "Does the Maximum Mean Discrepancy (MMD) method work for multivariate distributios\n",
    "\"\"\"\n",
    "\n",
    "response=ask_question(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the Maximum Mean Discrepancy (MMD) method works for multivariate distributions. In fact, it's commonly used and often preferred for comparing multivariate distributions over methods that rely on comparing marginal distributions.\n",
      "\n",
      "Here's why and how:\n",
      "\n",
      "* **MMD is designed for general distributions:**  The fundamental idea behind MMD is to embed the distributions into a reproducing kernel Hilbert space (RKHS).  This embedding allows MMD to compare distributions without needing to assume any specific parametric form or independence structure.  The RKHS framework is general enough to handle distributions of arbitrary dimensionality, including multivariate distributions.\n",
      "\n",
      "* **Capturing dependencies:**  By considering the joint distribution through the kernel function, MMD implicitly captures dependencies between the variables.  Methods that compare marginal distributions separately (e.g., comparing histograms of each variable independently) fail to capture these dependencies, which can lead to inaccurate comparisons if the variables are correlated.  MMD, with a suitable kernel, considers these dependencies, providing a more comprehensive measure of similarity between the distributions.\n",
      "\n",
      "* **Kernels define the comparison:** The choice of kernel function is crucial. Common choices for multivariate data include:\n",
      "    * **Gaussian (RBF) kernel:** `k(x, y) = exp(-||x - y||^2 / (2 * sigma^2))`  where `x` and `y` are multivariate data points, `||.||` is the Euclidean norm, and `sigma` is the bandwidth.  This kernel is widely used due to its simplicity and ability to capture smooth relationships.\n",
      "    * **Linear kernel:** `k(x, y) = x^T y`  This is suitable if a linear relationship between the variables is expected.\n",
      "    * **Polynomial kernel:** `k(x, y) = (x^T y + c)^d`  Offers more flexibility than the linear kernel.\n",
      "    * **Laplacian kernel:** `k(x, y) = exp(-||x - y|| / sigma)`  Less sensitive to outliers compared to the Gaussian kernel.\n",
      "    * **More complex kernels:**  Domain-specific kernels can be designed to incorporate prior knowledge about the data.\n",
      "\n",
      "* **MMD as a distance between embeddings:** The MMD calculates the distance between the mean embeddings of the two distributions in the RKHS.  This distance reflects the overall difference between the distributions, considering all variables and their relationships.\n",
      "\n",
      "* **Advantages for multivariate data:**\n",
      "    * **Captures dependencies:** As mentioned earlier, this is a key advantage.\n",
      "    * **Non-parametric:**  Doesn't require assuming a specific distribution family.\n",
      "    * **General purpose:**  Applicable to a wide range of multivariate distributions.\n",
      "    * **Relatively easy to implement:**  Many libraries provide MMD implementations.\n",
      "\n",
      "* **Considerations:**\n",
      "    * **Kernel selection:** Choosing the right kernel is crucial for performance. Experimentation and domain knowledge are important.\n",
      "    * **Bandwidth selection:** For kernels like the Gaussian kernel, the bandwidth parameter `sigma` needs to be chosen carefully. Cross-validation or heuristic methods can be used.\n",
      "    * **Computational cost:**  The computational cost can be significant for large datasets, as it involves pairwise kernel calculations.\n",
      "\n",
      "**In summary, MMD is a well-established and effective method for comparing multivariate distributions. Its ability to capture dependencies between variables and its non-parametric nature make it a powerful tool for various applications, including hypothesis testing, generative model evaluation, and domain adaptation.**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbiased precipitation estimates require both an optimal method and unbiased, artifact-free observations. Direct comparison to a reference dataset is the most reliable way to ensure unbiased observations. However, the number of direct intersects between GPM and TMRW-R1 is likely limited. Therefore, we will use an unpaired approach to evaluate TMRW-R1 observations against GPM observations. A common technique for comparing datasets is the Maximum Mean Discrepancy (MMD) method. MMD, a kernel-based method, measures the distance between dataset means in a reproducing kernel Hilbert space (RKHS). A small MMD indicates similar distributions. MMD is designed for general distributions, handles multivariate data, and captures inter-variable dependencies. However, MMD is sensitive to kernel function choice and can be computationally expensive for large datasets. Consequently, MMD alone is insufficient for evaluating TMRW-R1 observations. To avoid evaluating high-dimensional multivariate distributions, we propose developing and using an autoencoder model to learn a compressed representation of Ka-band radar observations. The autoencoder, trained using GPM Ka-band radar observations, will facilitate TMRW-R1 evaluation in two ways: (1) reconstruction errors will identify anomalous TMRW-R1 observations, and (2) the compressed representation will reduce data dimensionality, enabling the use of the MMD method.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"\n",
    "Could  you review following text?  Do not provide explanations or suggestions, just an edited version.\n",
    "\n",
    "In addition to an optimal method, unbiased and artifact-free observations are necessary to derive unbiased precipitation estimates. The most reliable method to ensure that observations are unbiased involves direct comparison reference dataset. However, the number of direct intersects between the GPM and TMRW-R1 is likely to be limited.  Therefore, we will use an unpaired approach to evaluate the TMRW-R1 observations using the GPM observations.  A general technique for comparing two different datasets is the Maximum Mean Discrepancy (MMD) method.  MMD is a kernel-based method that measures the distance between the means of the datasets in a reproducing kernel Hilbert space (RKHS). A small MMD suggests the distributions are similar.  The MMD is designed for general distributions and can handle multivariate data, capturing dependencies between variables.  The drawback is that the method is sensitive to the choice of the kernel function and can be computationally intensive for large datasets. As a consequence, the MMD method cannot be used as a standalone method for evaluating the TMRW-R1 observations.  To avoid the need to evaluate multivariate distributions with large dimensionality, we propose the development and use of an autoencoder model to learn a compressed representation of the Ka-band radar observations.  The autoencoder model will be trained using GPM Ka-band radar observations and learn a compressed representation of the Ka-band radar observations that can facilitate the evaluation of TMRW-R1 observations in two way.  First, the reconstruction errors of the autoencoder model will be used to evaluate to identify anomalous TMRW-R1 observations. Second, the compressed representation will significantly reduce the dimensionality of the data and allow the MMD method to be used to evaluate the TMRW-R1 observations.\n",
    "\"\"\"\n",
    "\n",
    "response=ask_question(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "Hi Mark, you may recall my expressing my interest in coordinating with \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
