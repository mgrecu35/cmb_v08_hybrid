{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "cpp"
    }
   },
   "outputs": [],
   "source": [
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <onnxruntime_c_api.h>\n",
    "\n",
    "// Global variables for the ONNX Runtime environment, session, and input/output tensors\n",
    "OrtEnv* env;\n",
    "OrtSession* session;\n",
    "OrtSessionOptions* session_options;\n",
    "//OrtAllocator* allocator;\n",
    "OrtValue* input_tensor;\n",
    "OrtValue* output_tensor;\n",
    "OrtMemoryInfo* memory_info;\n",
    "OrtStatus *status;\n",
    "// Initialization function\n",
    "OrtApi *g_api_conv_land;\n",
    "OrtApi *g_api_conv_ocean;\n",
    "OrtApi *g_api_strat_ocean;\n",
    "OrtApi *g_api_strat_land;\n",
    "\n",
    "void init_onnx_runtime_(void) {\n",
    "    // Initialize the ONNX Runtime environment\n",
    "    const OrtApi *api = OrtGetApiBase()->GetApi(ORT_API_VERSION);\n",
    "    const char* model_path = \"conv_and_strat_model_ku_jan28_2025.onnx\";\n",
    "    g_api = (OrtApi *)api;\n",
    "    status = g_api->CreateEnv(ORT_LOGGING_LEVEL_WARNING, \"test\", &env);\n",
    "    status = g_api->CreateSessionOptions(&session_options);\n",
    "    status = g_api->SetIntraOpNumThreads(session_options, 1);\n",
    "    status = g_api->CreateSession(env, model_path, session_options, &session);\n",
    "    if (status != NULL) {\n",
    "      const char* msg = g_api->GetErrorMessage(status);\n",
    "      fprintf(stderr, \"Failed to run ONNX model: %s\\n\", msg);\n",
    "      g_api->ReleaseStatus(status);\n",
    "    }\n",
    "    status = g_api->CreateCpuMemoryInfo(OrtArenaAllocator, OrtMemTypeDefault, &memory_info);\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "void call_onnx_(float *input_data, int *lengths_data, float *output_data, int *batch_size, int *seq_len, int *input_size, int *output_size) {\n",
    "    //'input': input_data, 'n_seq'\n",
    "    const char* input_names[] = {\"input\", \"n_seq\"};\n",
    "    const char* output_names[] = {\"output\"};\n",
    "\n",
    "    int64_t input_shape[3] = {(int64_t)(*batch_size), (int64_t)(*seq_len), (int64_t)(*input_size)};\n",
    "    size_t input_tensor_size = (*batch_size) * (*seq_len) * (*input_size);\n",
    "    OrtValue* input_tensor = NULL;\n",
    "\n",
    "    status=g_api->CreateTensorWithDataAsOrtValue(\n",
    "        memory_info, input_data, input_tensor_size * sizeof(float),\n",
    "        input_shape, 3, ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT, &input_tensor);\n",
    "    //printf(\"batch_size: %d\\n\", *batch_size);\n",
    "    //printf(\"seq_len: %d\\n\", *seq_len);\n",
    "    //printf(\"input_size: %d\\n\", *input_size);\n",
    "    //printf(\"output_size: %d\\n\", *output_size);\n",
    "    // Prepare seq_lengths tensor\n",
    "    int64_t lengths_shape[1] = {(int32_t)(*batch_size)};\n",
    "    OrtValue* lengths_tensor = NULL;\n",
    "    int32_t *lengths_data_t;\n",
    "    lengths_data_t = (int32_t *)malloc((*batch_size) * sizeof(int32_t));\n",
    "    for (int i = 0; i < *batch_size; i++) {\n",
    "        lengths_data_t[i] = (int32_t)lengths_data[i];\n",
    "    }\n",
    "    status=g_api->CreateTensorWithDataAsOrtValue(\n",
    "        memory_info, lengths_data_t, (*batch_size) * sizeof(int32_t),\n",
    "        lengths_shape, 1, ONNX_TENSOR_ELEMENT_DATA_TYPE_INT32, &lengths_tensor);\n",
    "    \n",
    "    if (status != NULL) {\n",
    "        const char* msg = g_api->GetErrorMessage(status);\n",
    "        fprintf(stderr, \"Failed to run ONNX model: %s\\n\", msg);\n",
    "        fprintf(stderr, \"batch_size: %d\\n\", *batch_size);\n",
    "        g_api->ReleaseStatus(status);\n",
    "    }\n",
    "    // Create input names and tensors\n",
    "    const OrtValue* input_tensors[] = {input_tensor, lengths_tensor};\n",
    "\n",
    "    // Prepare output tensor\n",
    "    int64_t output_shape[3] = {(int64_t)(*batch_size), (int64_t)(*seq_len), (int64_t)(*output_size)};\n",
    "    size_t output_tensor_size = (*batch_size) * (*seq_len) * (*output_size);\n",
    "    OrtValue* output_tensor = NULL;\n",
    "\n",
    "    status=g_api->CreateTensorWithDataAsOrtValue(\n",
    "        memory_info, output_data, output_tensor_size * sizeof(float),\n",
    "        output_shape, 3, ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT, &output_tensor);\n",
    "\n",
    "    \n",
    "    // Run the model\n",
    "    status = g_api->Run(\n",
    "        session, NULL, input_names, input_tensors, 2,\n",
    "        output_names, 1, &output_tensor);\n",
    "\n",
    "    if (status != NULL) {\n",
    "        const char* msg = g_api->GetErrorMessage(status);\n",
    "        fprintf(stderr, \"Failed to run ONNX model: %s\\n\", msg);\n",
    "        g_api->ReleaseStatus(status);\n",
    "    }\n",
    "\n",
    "    // Release resources\n",
    "    g_api->ReleaseValue(input_tensor);\n",
    "    g_api->ReleaseValue(lengths_tensor);\n",
    "    g_api->ReleaseValue(output_tensor);\n",
    "    free(lengths_data_t);\n",
    "}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
